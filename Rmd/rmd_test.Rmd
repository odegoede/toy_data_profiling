---
title: "rmd_test"
author: "Olivia de Goede"
date: "16/11/2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "C:/Users/ODEGOEDE/data_profiling")

library(tidyverse)
library(ggedit)

data_folder <- "toy_data"
fig_folder <- "figure"
out_folder <- "output"
```

## Intro

### Package info

These were the packages tested:
* [DataExplorer](http://boxuancui.github.io/DataExplorer/)
* [skimr](https://docs.ropensci.org/skimr/articles/skimr.html)
* [dlookr](https://github.com/choonghyunryu/dlookr)
* [validate](https://data-cleaning.github.io/validate/index.html)

All of these are CRAN packages, and they are all available on the HDP CRAN website.
<br />
<br />

## tl;dr

`DataExplorer`, `skimr`, and `dlookr` do fairly similar things (overall summaries of data, basic initial exploration). Of the three, `dlookr` was my preference; it had more thorough documentation, generated better quality plots, and the web report it makes looks a bit more slick than the ones made by the other packages.

The `validate` package lets the user create rules, and then tests those rules in the dataset. I can picture us packaging and sharing rules, e.g. postal code checks, date checks, etc., for users to apply to their own data that they've brought in (there are a couple of options for how to share rules described in this package's extensive documentation). Users could also improve on our rules and suggest new ones that we could add to our data profiling.

I think a combo of `dlookr` and `validate` would be a good starting point for data profiling in R.
<br />
<br />

## Set-up

### Install packages

```{r pkg_install, message=FALSE, warning=FALSE}
library(DataExplorer)
library(skimr)
library(dlookr)
library(validate)
```


### Toy datasets

I'm mostly using `fake_ppl`, which is a mock dataset I made of 1000 fake people with some "personal info" and some nonsensical numeric data. It incorporates some missingness, and the postal codes are deliberately messy.

`ergo` is from [calmcode](https://calmcode.io/datasets.html), and has a lot of numeric data that behaves like real data (body measurements of people). Some missingness.

`owid_covid` is COVID data (number of cases, hospitalizations, vaccinations, etc.), aggregated by geographic region. It's from [Our World in Data](https://github.com/owid/covid-19-data/tree/master/public/data).

`glob_mob` summarizes global mobility: change in travel during COVID, aggregated by geographic region. It's a [Google dataset](https://www.google.com/covid19/mobility/). It is a very big dataset, so it's good for testing the limits of these packages (7.5 million rows).


```{r data_load}
fake_ppl <- read_csv(file = "toy_data/mockaroo_edited_MOCK_DATA.csv",
                     show_col_types = FALSE)
ergo <- read_csv(file = "toy_data/calmcode_ergonomics.csv",
                 show_col_types = FALSE)
owid_covid <- read_csv(file = "toy_data/our_world_in_data_covid_data.csv",
                       show_col_types = FALSE)
glob_mob <- read_csv(file = "toy_data/google_Global_Mobility_Report.csv",
                     show_col_types = FALSE)
tibble(dataset = c("fake_ppl", "ergo", "owid_covid", "glob_mob"),
       n_rows = c(nrow(fake_ppl), nrow(ergo), nrow(owid_covid), nrow(glob_mob)), 
       n_cols = c(ncol(fake_ppl), ncol(ergo), ncol(owid_covid), ncol(glob_mob)))
```
<br />
<br />

## 1. DataExplorer

### Examples of tables and graphs made by this package

`introduce` produces a general overview table of your data:
```{r dataExp_introduce}
introduce(fake_ppl)
t(introduce(fake_ppl))
t(introduce(ergo))
t(introduce(owid_covid))
t(introduce(glob_mob))
```
There is also a plot form of this, which you can see in the final reports, but IMO the table form is more compact and useful.

<br />

`plot_missing` makes a handy plot of the number of missing values for each field.
```{r dataExp_missing}
plot_missing(fake_ppl)
plot_missing(ergo)
plot_missing(owid_covid)
plot_missing(glob_mob)
```

<br />
